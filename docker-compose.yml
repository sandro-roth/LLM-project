services:
  inference:
    build:
      context: .
      dockerfile: LLMs/Inference.Dockerfile
      args:
        USE_PROXY: ${USE_PROXY}
        HTTP_PROXY: ${HTTP_PROXY}
        HTTPS_PROXY: ${HTTPS_PROXY}
        NO_PROXY: ${NO_PROXY}
        LOG_DIR: ${LOG_DIR}
    container_name: mistral-inference-app
    ports:
      - "8100:8100"
    restart: "no"
    volumes:
      - ./LLMs/Mistral7B/mistral-7B-Instruct-v0.3:/app/mistral-7B-Instruct-v0.3
      - ./LLMs/Mistral7B/mistral-7B-v0.1:/app/mistral-7B-v0.1
    gpus: all
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    # === Limits (hinzugefügt) ===
    cpus: "8"
    mem_limit: "32g"
    mem_reservation: "16g"
    pids_limit: 2048
    ulimits:
      nofile: 65536

  streamlit-web:
    build:
      context: .
      dockerfile: webinterface/Dockerfile
      args:
        USE_PROXY: ${USE_PROXY}
        HTTP_PROXY: ${HTTP_PROXY}
        HTTPS_PROXY: ${HTTPS_PROXY}
        NO_PROXY: ${NO_PROXY}
        LOG_DIR: ${LOG_DIR}
    container_name: streamlit-web-app
    ports:
      - "8501:8501"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - STREAMLIT_SERVER_PORT=8501
      - STREAMLIT_SERVER_HEADLESS=true
      - API_BASE_URL_MISTRAL=${API_BASE_URL_MISTRAL}
      - API_BASE_URL_MEDITRON=${API_BASE_URL_MEDITRON}
    depends_on:
      - mistral-inference
    # === Limits (hinzugefügt) ===
    cpus: "2"
    mem_limit: "2g"
    mem_reservation: "1g"
    pids_limit: 512
    ulimits:
      nofile: 8192
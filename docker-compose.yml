services:
  inference:
    build:
      context: .
      dockerfile: LLMs/Inference.Dockerfile
      target: ${DOCKER_INFERENCE}
      args:
        DOCKER_INFERENCE: ${DOCKER_INFERENCE}
        USE_PROXY: ${USE_PROXY}
        HTTP_PROXY: ${HTTP_PROXY}
        HTTPS_PROXY: ${HTTPS_PROXY}
        NO_PROXY: ${NO_PROXY}
        LOG_DIR: ${LOG_DIR}
    container_name: llm-inference
    ports:
      - "8100:8100"
    restart: "no"
    volumes:
      - ./LLMs/Mistral7B/mistral-7B-Instruct-v0.3:/app/mistral-7B-Instruct-v0.3
      - ./LLMs/Meditron7B/model:/app/model
      - ./LLMs/Apertus8B/base_model:/app/base_model
      - ./LLMs/Qwen3/model_inference:/app/model_inference
      - ./LLMs/Qwen3/model_thinking:/app/model_thinking
    gpus: all
    environment:
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES}
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HF_LOCAL_ONLY=${HF_LOCAL_ONLY}
      - TORCH_DTYPE=${TORCH_DTYPE}
      - MAX_VRAM_PER_GPU=${MAX_VRAM_PER_GPU}
      - CPU_RAM_BUDGET_GIB=${CPU_RAM_BUDGET_GIB}
      - LOAD_IN_4BIT=${LOAD_IN_4BIT}
      - LOAD_IN_8BIT=${LOAD_IN_8BIT}
      - OFFLOAD_FOLDER=/app/offload
    healthcheck:
      test: ["CMD-SHELL","curl -sf http://localhost:8100/health || curl -sf http://localhost:8100/config || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 20

    # === Limits (hinzugefügt) ===
    #cpus: "8"
    #mem_limit: "32g"
    #mem_reservation: "16g"
    #pids_limit: 2048
    #ulimits:
    #  nofile: 65536

  streamlit-web:
    depends_on:
      inference:
        condition: service_healthy
    build:
      context: .
      dockerfile: webinterface/Dockerfile
      args:
        USE_PROXY: ${USE_PROXY}
        HTTP_PROXY: ${HTTP_PROXY}
        HTTPS_PROXY: ${HTTPS_PROXY}
        NO_PROXY: ${NO_PROXY}
        LOG_DIR: ${LOG_DIR}
    container_name: streamlit-web-app
    ports:
      - "8501:8501"
    environment:
      - STREAMLIT_SERVER_PORT=8501
      - STREAMLIT_SERVER_HEADLESS=true
      - API_BASE_URL=${API_BASE_URL}
      - DOCKER_INFERENCE=${DOCKER_INFERENCE}
      - STREAMLIT_MODEL_SELECT=${STREAMLIT_MODEL_SELECT}

    # === Limits (hinzugefügt) ===
    cpus: "2"
    mem_limit: "2g"
    mem_reservation: "1g"
    pids_limit: 512
    ulimits:
      nofile: 8192
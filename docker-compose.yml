services:
  inference:
    build:
      context: .
      dockerfile: LLMs/Inference.Dockerfile
      args:
        DOCKER_INFERENCE: ${DOCKER_INFERENCE}
        USE_PROXY: ${USE_PROXY}
        HTTP_PROXY: ${HTTP_PROXY}
        HTTPS_PROXY: ${HTTPS_PROXY}
        NO_PROXY: ${NO_PROXY}
        LOG_DIR: ${LOG_DIR}
    container_name: llm-inference
    ports:
      - "8100:8100"
    restart: "no"
    volumes:
      - ./LLMs/Mistral7B/mistral-7B-Instruct-v0.3:/app/mistral-7B-Instruct-v0.3
      - ./LLMs/Meditron7B/model:/app/model
      - ./LLMs/Apertus8B/base_model:/app/base_model
    gpus: all
    environment:
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES}
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    # === Limits (hinzugefügt) ===
    cpus: "8"
    mem_limit: "32g"
    mem_reservation: "16g"
    pids_limit: 2048
    ulimits:
      nofile: 65536

  streamlit-web:
    build:
      context: .
      dockerfile: webinterface/Dockerfile
      args:
        USE_PROXY: ${USE_PROXY}
        HTTP_PROXY: ${HTTP_PROXY}
        HTTPS_PROXY: ${HTTPS_PROXY}
        NO_PROXY: ${NO_PROXY}
        LOG_DIR: ${LOG_DIR}
    container_name: streamlit-web-app
    ports:
      - "8501:8501"
    environment:
      - STREAMLIT_SERVER_PORT=8501
      - STREAMLIT_SERVER_HEADLESS=true
      - API_BASE_URL=${API_BASE_URL}
      - DOCKER_INFERENCE=${DOCKER_INFERENCE}
      - STREAMLIT_MODEL_SELECT=${STREAMLIT_MODEL_SELECT}

    # === Limits (hinzugefügt) ===
    cpus: "2"
    mem_limit: "2g"
    mem_reservation: "1g"
    pids_limit: 512
    ulimits:
      nofile: 8192